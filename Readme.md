# Ejecutar proyecto

## Requisitos previos
1. **Docker** y **Docker Desktop** deben estar instalados en su sistema.
2. Tener **Node.js** y **npm** instalados para la gesti√≥n de dependencias.
3. Tener **Mysql Workbench** o **Algun gestor de base de datos que permita MYSQL**
---

## Instrucciones de instalaci√≥n y ejecuci√≥n

1. **Clonar el repositorio**
   ```bash
   git clone https://github.com/SebaVRdev/captahydro-test.git
   ```

2. **Acceder a la carpeta del proyecto**
   ```bash
   cd captahydro-test
   ```

3. **Instalar dependencias**
   ```bash
   npm install
   ```
4. **Copiar variables de entorno**
    ```bash
    cp .env.dev .env
    ```

5. **Levantar la aplicaci√≥n con Docker**
   ```bash
   docker-compose up -d
   ```

6. **Esperar a que todos los contenedores se levanten correctamente.**

---

## Conexi√≥n a la base de datos

- La base de datos MySQL se encuentra disponible en el puerto **3305** (diferente del puerto por defecto **3306** de MySQL local).
- Datos de conexi√≥n:
  - **Host:** localhost
  - **Puerto:** 3305
  - **Usuario:** root
  - **Contrase√±a:** seba123

**üëÅ‚Äçüó® Prueba**
```bash
mysql -h 127.0.0.1 -P 3305 -u root -p
```

Si tienes MySQL instalado localmente, aseg√∫rate de no tener conflictos de puertos con el 3306.

---

## Acceso a la API
- La API estar√° disponible en la URL:
  ```
  http://localhost:3000/api
  ```

---

‚ö†Ô∏è **NOTA IMPORTANTE:**
Los enpoints para solicitar informaci√≥n requieren de que la base de datos est√© poblada.
Se recomienda ejecutar el endpoint del scraper con las fechas necesarias para guardar en base de datos. 

## Endpoints disponibles
A continuaci√≥n se describen los cuatro endpoints principales de la API.

1. **POST /scrape**
   - **Descripci√≥n:** Ejecuta el proceso de scraping en la fuente.
   - **Par√°metros de entrada:** 
   ```json
     {
        "fecha_inicio": "YYYY-MM-DD",
        "fecha_fin": "YYYY-MM-DD",
        "obras": ["codigo_obra"]
     }
    ```
    **En caso de no mandar ningun parametro, el scraper se ejecuta con la fecha de los ultimos 2 dias y con un set de obras por defecto.**
   - **Respuesta de ejemplo:** Entrega la informacion obtenida y ok: boolean | true en caso de que se haya guardado la informaci√≥n exitosamente.
 
2. **GET /stations**
   - **Descripci√≥n:** Entrega todas las obras con su ultimo caudal.
   - **Par√°metros de entrada:** Ninguno
   - **Respuesta de ejemplo:** Lista de todos las obras

3. **GET /stations/:codigo_obra**
   - **Descripci√≥n:** Detalle de la obra.
   - **Par√°metros de entrada:** codigo_obra
   - **Respuesta de ejemplo:**
     ```json
     {
        "ok": true,
        "data": {
            "id": 1,
            "codigo_obra": "OB-1303-1072",
            "ultimo_caudal": 8268.66015625,
            "region": "Metropolitana",
            "provincia": "Maipo",
            "comuna": "Buin",
            "coordenada_norte": 6276027,
            "coordenada_este": 348488,
            "created_at": "2024-12-13T14:51:21.000Z"
        }
    }
     ```

4. **GET /stations/:codigo_obra/flow**
   - **Descripci√≥n:** Entrega todos los caudales de una obra entre un rango de fechas.
   - **Par√°metros de entrada:** codigo_obra, fecha_inicio (YYYY-MM-DD) y fecha_fin (YYYY-MM-DD)
   ```json
     {
        "fecha_inicio": "YYYY-MM-DD",
        "fecha_fin": "YYYY-MM-DD"
    }
    ```
    **En caso de no mandar ningun parametro, el scraper se ejecuta con la fecha de los ultimos 2 dias y con un set de obras por defecto.**
   - **Respuesta de ejemplo:**
     ```json
     {
       "ok": true,
        "caudales": [
            {
                "id": 1,
                "codigo_obra": "OB-1303-1072",
                "caudal": 8125.33984375,
                "altura_limnimetrica": 110,
                "fecha_hora_medicion": "2024-12-11T00:00:00.000Z"
            },
        ]
     }
     ```

---

## Notas importantes
### En caso de fallar usando docker prueba creando la base de datos manualmente
```bash
mysql -h 127.0.0.1 -P 3306 -u root -p
```
**Dentro de la consola de MYSQL ejecuta...**
```bash
source /ruta/completa/al/archivo/database/init.sql;
```
**Con esto se crea la base de datos, por lo que se levanta el proyecto manualmente**
```bash
npm run dev
```
Para cualquier problema o duda, por favor comun√≠quese a mi numero de telefono +56 958138659. 
